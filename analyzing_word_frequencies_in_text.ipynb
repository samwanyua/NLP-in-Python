{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyzing Word Frequencies in Text\n",
    "Objectives:\n",
    "- Calculating basic statistics of word frequencies\n",
    "- Visualize the frequency distribution of common words\n",
    "- Compare observed word frequencies with expected distributions\n",
    "- Draw insights about word usage pattern in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/wanyua/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Load and preprocess text data\n",
    "import re # allows you to use regular expressions (regex) in Python.\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "import spacy\n",
    "import gensim\n",
    "import string\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "from wordcloud import WordCloud\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from textblob import TextBlob\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from gensim import corpora\n",
    "from gensim.models import LdaModel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/wanyua/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/wanyua/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/wanyua/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Error loading average_perceptron_tagger: Package\n",
      "[nltk_data]     'average_perceptron_tagger' not found in index\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: en-core-web-sm==3.5.0 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.5.0/en_core_web_sm-3.5.0-py3-none-any.whl in ./env/lib/python3.8/site-packages (3.5.0)\n",
      "Requirement already satisfied: spacy<3.6.0,>=3.5.0 in ./env/lib/python3.8/site-packages (from en-core-web-sm==3.5.0) (3.5.3)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in ./env/lib/python3.8/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.32.3)\n",
      "Requirement already satisfied: jinja2 in ./env/lib/python3.8/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.1.5)\n",
      "Requirement already satisfied: packaging>=20.0 in ./env/lib/python3.8/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (24.2)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in ./env/lib/python3.8/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.0.12)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in ./env/lib/python3.8/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.0.10)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in ./env/lib/python3.8/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.0.12)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in ./env/lib/python3.8/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.1.3)\n",
      "Requirement already satisfied: setuptools in ./env/lib/python3.8/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (44.0.0)\n",
      "Requirement already satisfied: numpy>=1.15.0 in ./env/lib/python3.8/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.24.4)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in ./env/lib/python3.8/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (8.1.12)\n",
      "Requirement already satisfied: pathy>=0.10.0 in ./env/lib/python3.8/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.11.0)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in ./env/lib/python3.8/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (6.4.0)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in ./env/lib/python3.8/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.0.11)\n",
      "Requirement already satisfied: typer<0.8.0,>=0.3.0 in ./env/lib/python3.8/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.7.0)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in ./env/lib/python3.8/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.0.5)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in ./env/lib/python3.8/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.4.8)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in ./env/lib/python3.8/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (4.67.1)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in ./env/lib/python3.8/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.4.1)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in ./env/lib/python3.8/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.0.9)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in ./env/lib/python3.8/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.10.21)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./env/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.4.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./env/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./env/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2025.1.31)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./env/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.10)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./env/lib/python3.8/site-packages (from jinja2->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.1.5)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in ./env/lib/python3.8/site-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.7.11)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in ./env/lib/python3.8/site-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.1.5)\n",
      "Requirement already satisfied: pathlib-abc==0.1.1 in ./env/lib/python3.8/site-packages (from pathy>=0.10.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.1.1)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in ./env/lib/python3.8/site-packages (from typer<0.8.0,>=0.3.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (8.1.8)\n",
      "Requirement already satisfied: language-data>=1.2 in ./env/lib/python3.8/site-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.3.0)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in ./env/lib/python3.8/site-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (4.12.2)\n",
      "Requirement already satisfied: marisa-trie>=1.1.0 in ./env/lib/python3.8/site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.2.1)\n",
      "\u001b[38;5;2mâœ” Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "# Load SpaCy's English model\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('average_perceptron_tagger')\n",
    "spacy.cli.download(\"en_core_web_sm\")\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample text for analysis\n",
    "text = \"\"\"\n",
    "Over the past decade, smart devices have transformed the way we interact with technology. From smartphones to smart home assistants, these gadgets have become an essential part of our daily lives.\n",
    "Many users appreciate the convenience and efficiency that these devices offer. One customer stated, \"My smart speaker has completely changed how I control my home. I can now adjust the lights, play music, and even check the weather using just my voice!\"\n",
    "However, some users raise concerns about privacy and data security. A user on a tech forum commented, \"I love the features, but I worry about my personal data being collected without my consent.\" This has led to discussions on the ethical implications of AI-driven smart devices.\n",
    "Another major factor in customer satisfaction is battery life. While high-end devices promise long-lasting battery performance, some users report inconsistent battery life after months of usage. A reviewer mentioned, \"My smartwatch was great for the first three months, but now I need to charge it twice a day.\"\n",
    "Despite these concerns, the global demand for smart devices continues to grow. Market research suggests that by 2030, the number of connected devices worldwide will exceed 50 billion. Brands are investing in AI, better privacy controls, and sustainable tech to improve customer trust.\n",
    "The question remains: Are smart devices making our lives easier, or are we becoming too dependent on technology?\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Preprocessed Tokens: ['past', 'decade', 'smart', 'device', 'transformed', 'way', 'interact', 'smartphones', 'smart', 'home', 'assistant', 'gadget', 'become', 'essential', 'part', 'daily', 'many', 'user', 'appreciate', 'convenience', 'efficiency', 'device', 'one', 'customer', 'stated', 'smart', 'speaker', 'completely', 'changed', 'control', 'adjust', 'light', 'play', 'music', 'even', 'check', 'weather', 'using', 'voice', 'however', 'user', 'raise', 'concern', 'privacy', 'data', 'user', 'tech', 'forum', 'commented', 'love', 'feature', 'worry', 'personal', 'data', 'collected', 'without', 'led', 'discussion', 'ethical', 'implication', 'smart', 'another', 'major', 'factor', 'customer', 'satisfaction', 'battery', 'device', 'promise', 'battery', 'performance', 'user', 'report', 'inconsistent', 'battery', 'life', 'month', 'reviewer', 'mentioned', 'smartwatch', 'great', 'first', 'three', 'month', 'need', 'charge', 'twice', 'despite', 'concern', 'global', 'demand', 'smart', 'device', 'continues', 'market', 'research', 'suggests', 'number', 'connected', 'device', 'worldwide', 'exceed', 'brand', 'investing', 'ai', 'better', 'privacy', 'control', 'sustainable', 'tech', 'improve', 'customer', 'question', 'remains', 'smart', 'device', 'making', 'life', 'easier', 'becoming', 'dependent', 'technology']\n"
     ]
    }
   ],
   "source": [
    "# Preprocessing\n",
    "def preprocess_text(text, use_lemmatization=True):\n",
    "    tokenizer = TreebankWordTokenizer()\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [token.lower() for token in tokens if token.lower() not in stop_words and token.isalpha()]\n",
    "\n",
    "    if use_lemmatization:\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    else:\n",
    "        stemmer = PorterStemmer()\n",
    "        tokens = [stemmer.stem(token) for token in tokens]\n",
    "\n",
    "    return tokens\n",
    "\n",
    "print(\"\\nPreprocessed Tokens:\", preprocess_text(text))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment Analysis\n",
    "Sentiment Analysis is a Natural Language Processing (NLP) technique used to determine the emotional tone behind a body of text. It classifies text into positive, negative, or neutral sentiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sentiment Analysis: Positive\n"
     ]
    }
   ],
   "source": [
    "# Sentiment analysis\n",
    "def sentiment_analysis(text):\n",
    "    blob = TextBlob(text)\n",
    "    sentiment_score = blob.sentiment.polarity  # Ranges from -1 (negative) to 1 (positive)\n",
    "    if sentiment_score > 0:\n",
    "        return \"Positive\"\n",
    "    elif sentiment_score < 0:\n",
    "        return \"Negative\"\n",
    "    else:\n",
    "        return \"Neutral\"\n",
    "    \n",
    "print(\"\\nSentiment Analysis:\", sentiment_analysis(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Named Entity Recogntion (NER)\n",
    "Named Entity Recognition (NER) is a Natural Language Processing (NLP) technique used to identify and classify key entities in a text into predefined categories. These entities could be:\n",
    "\n",
    "- Persons â€“ \"Elon Musk\", \"Albert Einstein\"\n",
    "- Organizations â€“ \"Google\", \"NASA\", \"Harvard University\"\n",
    "- Locations â€“ \"New York\", \"Mount Everest\", \"Africa\"\n",
    "- Dates & Time â€“ \"January 1, 2024\", \"last Monday\"\n",
    "- Monetary Values â€“ \"$100\", \"10 euros\"\n",
    "- Products â€“ \"iPhone 15\", \"Tesla Model S\"\n",
    "- Events â€“ \"World Cup\", \"Olympics 2024\"\n",
    "- Other Entities â€“ Percentages, Laws, Medical Conditions, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Named Entities: {'the past decade': 'DATE', 'One': 'CARDINAL', 'months': 'DATE', 'the first three months': 'DATE', '2030': 'DATE', '50 billion': 'MONEY', 'AI': 'ORG'}\n"
     ]
    }
   ],
   "source": [
    "# Named Entity Recognition(NER)\n",
    "def extract_entities(text):\n",
    "    doc = nlp(text)\n",
    "    entities = {ent.text: ent.label_ for ent in doc.ents}\n",
    "    return entities\n",
    "\n",
    "print(\"\\nNamed Entities:\", extract_entities(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topic Modeling\n",
    "Topic modeling is an unsupervised machine learning technique used to identify hidden themes or topics in a collection of documents. It is commonly used in text analysis, document clustering, and content classification.\n",
    "\n",
    "**Why use Topic Modeling?**\n",
    "- Summarize large volumes of text (e.g., news articles, research papers)\n",
    "- Organize and categorize documents (e.g., customer feedback, legal documents)\n",
    "- Extract key themes from unstructured text\n",
    "\n",
    "**Common Topic Modeling Techniques**\n",
    "- Latent Dirichlet Allocation (LDA) â€“ A probabilistic model that assumes each document is a mixture of topics\n",
    "- Non-Negative Matrix Factorization (NMF) â€“ Uses matrix factorization for topic discovery\n",
    "- Latent Semantic Analysis (LSA) â€“ Uses Singular Value Decomposition (SVD) to detect topics\n",
    "- BERTopic â€“ A deep-learning-based approach using transformer embedding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topic Modeling: [(0, '0.011*\"smart\" + 0.010*\"device\" + 0.010*\"user\" + 0.010*\"battery\" + 0.010*\"customer\" + 0.010*\"tech\" + 0.010*\"concern\" + 0.010*\"data\" + 0.010*\"privacy\" + 0.010*\"month\"'), (1, '0.038*\"device\" + 0.038*\"smart\" + 0.026*\"user\" + 0.020*\"customer\" + 0.020*\"battery\" + 0.015*\"life\" + 0.015*\"control\" + 0.015*\"month\" + 0.015*\"privacy\" + 0.015*\"data\"')]\n"
     ]
    }
   ],
   "source": [
    "def topic_modeling(text):\n",
    "    tokens = preprocess_text(text)\n",
    "    dictionary = corpora.Dictionary([tokens])\n",
    "    corpus = [dictionary.doc2bow(tokens)]\n",
    "    lda_model = LdaModel(corpus, num_topics=2, id2word=dictionary, passes=10)\n",
    "    return lda_model.print_topics()\n",
    "print(\"\\nTopic Modeling:\", topic_modeling(text))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Two Topics Identified: The model extracted two main topics from the text.\n",
    "- Each Topic is a Collection of Words:\n",
    "    - Each topic consists of words with associated weights (probabilities).\n",
    "    - The weight represents how strongly the word contributes to that topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top Word Frequencies: [('smart', 6), ('device', 6), ('user', 4), ('customer', 3), ('battery', 3), ('control', 2), ('concern', 2), ('privacy', 2), ('data', 2), ('tech', 2)]\n"
     ]
    }
   ],
   "source": [
    "# Word frequency analysis\n",
    "def word_frequencies(text):\n",
    "    tokens = preprocess_text(text)\n",
    "    word_counts = Counter(tokens)\n",
    "    return word_counts.most_common(10)\n",
    "\n",
    "print(\"\\nTop Word Frequencies:\", word_frequencies(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
